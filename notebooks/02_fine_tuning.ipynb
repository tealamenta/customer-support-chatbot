{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Fine-Tuning avec LoRA \n",
    "\n",
    "**Objectif**: Fine-tuner un LLM sur le dataset Customer Support\n",
    "\n",
    "**Modele**: TinyLlama-1.1B (optimise pour Mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# !pip install torch transformers datasets accelerate peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "Using MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Check device\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    print('Using MPS (Apple Silicon)')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print(f'Using CUDA: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('Using CPU - training will be slow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODEL_DIR = DATA_DIR / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model - TinyLlama optimise pour Mac M1\n",
    "BASE_MODEL = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training config (reduit pour Mac)\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Subset pour entrainement rapide (optionnel)\n",
    "USE_SUBSET = True\n",
    "SUBSET_SIZE = 2000\n",
    "\n",
    "print('Config loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Charger les donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2000\n",
      "Val samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Charger les donnees preparees\n",
    "with open(DATA_DIR / 'processed' / 'train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(DATA_DIR / 'processed' / 'val.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Utiliser subset pour test rapide\n",
    "if USE_SUBSET:\n",
    "    train_data = train_data[:SUBSET_SIZE]\n",
    "    val_data = val_data[:500]\n",
    "\n",
    "# Convertir en Dataset HuggingFace\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc6baa38934f30b55a8604a4d73c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faca533be8a646debdfe567b8219c3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple formate:\n",
      "<|system|>\n",
      "You are a helpful customer support assistant.</s>\n",
      "<|user|>\n",
      "want help seeing the early termination fee</s>\n",
      "<|assistant|>\n",
      "To help you understand the early termination fee, please provide us with your account details so that we can access the specific terms and conditions associated with your plan. Once we have this information, we will be able to provide you with a detailed explanation of\n"
     ]
    }
   ],
   "source": [
    "# Reformater pour TinyLlama (format different de Mistral)\n",
    "def format_for_tinyllama(example):\n",
    "    # Extraire instruction et response du format actuel\n",
    "    text = example['text']\n",
    "    \n",
    "    # Parser le format existant\n",
    "    if 'Customer:' in text and 'Assistant:' in text:\n",
    "        customer_part = text.split('Customer:')[1].split('[/INST]')[0].strip()\n",
    "        assistant_part = text.split('Assistant:')[1].replace('</s>', '').strip()\n",
    "    else:\n",
    "        return example\n",
    "    \n",
    "    # Format TinyLlama\n",
    "    formatted = f\"\"\"<|system|>\n",
    "You are a helpful customer support assistant.</s>\n",
    "<|user|>\n",
    "{customer_part}</s>\n",
    "<|assistant|>\n",
    "{assistant_part}</s>\"\"\"\n",
    "    \n",
    "    return {'text': formatted}\n",
    "\n",
    "# Appliquer le format\n",
    "train_dataset = train_dataset.map(format_for_tinyllama)\n",
    "val_dataset = val_dataset.map(format_for_tinyllama)\n",
    "\n",
    "print('\\nExemple formate:')\n",
    "print(train_dataset[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Charger le modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(f'Tokenizer: {tokenizer.name_or_path}')\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Model loaded!\n",
      "Parameters: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "# Charger le modele\n",
    "print(f'Loading model: {BASE_MODEL}')\n",
    "\n",
    "# Sur Mac (MPS), device_map='auto' peut causer des problemes avec PEFT/LoRA\n",
    "# (Erreur: expected device meta but got mps:0)\n",
    "# Il vaut mieux charger explicitement sur le device\n",
    "if DEVICE == 'mps':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float32, # Utiliser torch_dtype pour compatibilite\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "print('Model loaded!')\n",
    "print(f'Parameters: {model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configurer LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 4,505,600\n",
      "Total parameters: 1,104,553,984\n",
      "Trainable: 0.41%\n"
     ]
    }
   ],
   "source": [
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    ")\n",
    "\n",
    "# Appliquer LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Stats\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable: {100 * trainable_params / total_params:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments ready\n"
     ]
    }
   ],
   "source": [
    "# Training arguments optimises pour Mac M1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_DIR / 'checkpoints'),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=2,\n",
    "    # Mac M1 specific\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    # use_mps_device=(DEVICE == 'mps'), # Deprecated in newer transformers\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "print('Training arguments ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea4848fcc624e46a6f256900e8d3851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa240dfda1a47499b1e5920c1a311a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3f94045b62435186e1a138c11e633c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b5b536b4e1497bbf018216d4ac4812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeda00d2179405a83063dd2203fefdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77ad49d363e483ab5d11ba129828818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=str(MODEL_DIR / 'checkpoints'),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print('Trainer ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Train samples: 2000\n",
      "Epochs: 1\n",
      "Batch size: 2 x 8 = 16\n",
      "Learning rate: 0.0002\n",
      "Device: mps\n",
      "============================================================\n",
      "\n",
      "Training ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 1:09:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.9846302795410157, metrics={'train_runtime': 4194.8234, 'train_samples_per_second': 0.477, 'train_steps_per_second': 0.03, 'total_flos': 2842478903476224.0, 'train_loss': 0.9846302795410157})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print('='*60)\n",
    "print('STARTING TRAINING')\n",
    "print('='*60)\n",
    "print(f'Model: {BASE_MODEL}')\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Epochs: {NUM_EPOCHS}')\n",
    "print(f'Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION}')\n",
    "print(f'Learning rate: {LEARNING_RATE}')\n",
    "print(f'Device: {DEVICE}')\n",
    "print('='*60)\n",
    "print('\\nTraining ...\\n')\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../data/models/customer-support-tinyllama-lora\n"
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "final_model_path = MODEL_DIR / 'customer-support-tinyllama-lora'\n",
    "trainer.save_model(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "print(f'Model saved to: {final_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test le modele fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function ready\n"
     ]
    }
   ],
   "source": [
    "def generate_response(question):\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful customer support assistant.</s>\n",
    "<|user|>\n",
    "{question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extraire la reponse\n",
    "    if '<|assistant|>' in response:\n",
    "        response = response.split('<|assistant|>')[-1].strip()\n",
    "    return response\n",
    "\n",
    "print('Generation function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST RESPONSES\n",
      "============================================================\n",
      "\n",
      "Customer: I want to cancel my order\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I's, or other or in the other customers in the future.\n",
      "<|user|<| < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <\n",
      "----------------------------------------\n",
      "\n",
      "Customer: Where is my package?\n",
      "Assistant: We have to help you's < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <\n",
      "----------------------------------------\n",
      "\n",
      "Customer: How do I get a refund?\n",
      "Assistant: I could be a unique customer service\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<| < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <\n",
      "----------------------------------------\n",
      "\n",
      "Customer: I received a damaged product\n",
      "Assistant: Thank you can be a wakeholders in the b < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <\n",
      "----------------------------------------\n",
      "\n",
      "Customer: What is your return policy?\n",
      "Assistant: I's that we can help you want to have a lot of the most helpful to the 16, and he knew the other, \n",
      "\n",
      "<|<|<|assistant< < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    'I want to cancel my order',\n",
    "    'Where is my package?',\n",
    "    'How do I get a refund?',\n",
    "    'I received a damaged product',\n",
    "    'What is your return policy?',\n",
    "]\n",
    "\n",
    "print('='*60)\n",
    "print('TEST RESPONSES')\n",
    "print('='*60)\n",
    "for q in test_questions:\n",
    "    print(f'\\nCustomer: {q}')\n",
    "    print(f'Assistant: {generate_response(q)}')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I want to cancel my order\n",
      "A: Thank you can help us, and the company's of the 201:\n",
      "----------------------------------------\n",
      "Q: Where is my package?\n",
      "A: I can help you have to the company's, and other areas of the following:\n",
      "----------------------------------------\n",
      "Q: How do I get a refund?\n",
      "A: I's, and the other areas of the 198-0.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_response(question):\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful customer support assistant.</s>\n",
    "<|user|>\n",
    "{question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,  # Evite les repetitions\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if '<|assistant|>' in response:\n",
    "        response = response.split('<|assistant|>')[-1].strip()\n",
    "    # Nettoyer les caracteres bizarres\n",
    "    response = response.split('<')[0].strip()\n",
    "    return response\n",
    "\n",
    "# Test\n",
    "for q in ['I want to cancel my order', 'Where is my package?', 'How do I get a refund?']:\n",
    "    print(f'Q: {q}')\n",
    "    print(f'A: {generate_response(q)}')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarder les metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved!\n",
      "{\n",
      "  \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"lora_r\": 16,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"train_samples\": 2000,\n",
      "  \"val_samples\": 500,\n",
      "  \"epochs\": 1,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"trainable_params\": 4505600,\n",
      "  \"trainable_percent\": 0.40791125334440875,\n",
      "  \"device\": \"mps\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les resultats\n",
    "results = {\n",
    "    'model': BASE_MODEL,\n",
    "    'lora_r': LORA_R,\n",
    "    'lora_alpha': LORA_ALPHA,\n",
    "    'train_samples': len(train_dataset),\n",
    "    'val_samples': len(val_dataset),\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'trainable_params': trainable_params,\n",
    "    'trainable_percent': 100 * trainable_params / total_params,\n",
    "    'device': DEVICE,\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / 'results' / 'training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('Results saved!')\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
